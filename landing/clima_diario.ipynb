{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9be96a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la descarga de datos de AEMET...\n",
      " Iniciando descarga del período: 2020-01-01 a 2020-01-30\n",
      "Se procesarán 2 intervalos de 15 días\n",
      "\n",
      "Procesando intervalo 1/2: 2020-01-01 a 2020-01-15\n",
      "Descargados 12903 registros (Total acumulado: 12903)\n",
      "\n",
      "Procesando intervalo 2/2: 2020-01-16 a 2020-01-30\n",
      "Descargados 12894 registros (Total acumulado: 25797)\n",
      "\n",
      " Archivo guardado como: aemet_unificado_2020-01-01_2020-01-30.json\n",
      "Estadísticas:\n",
      "- Registros totales: 25797\n",
      "- Estaciones únicas: 867\n",
      "\n",
      "Cargando archivo aemet_unificado_2020-01-01_2020-01-30.json a Spark...\n",
      "Warehouse configurado: ../data/warehouse\n",
      "\n",
      " Procesando JSON a Spark...\n",
      "\n",
      " Datos cargados correctamente...\n",
      "\n",
      "Total registros: 25797\n",
      "+----------+----------+--------------------+---------------------+-------+----+----+----+--------+----+--------+----+--------+-----+---------+-------+-----+---------+-----+---------+\n",
      "|fecha     |indicativo|nombre              |provincia            |altitud|tmed|prec|tmin|horatmin|tmax|horatmax|dir |velmedia|racha|horaracha|hrMedia|hrMax|horaHrMax|hrMin|horaHrMin|\n",
      "+----------+----------+--------------------+---------------------+-------+----+----+----+--------+----+--------+----+--------+-----+---------+-------+-----+---------+-----+---------+\n",
      "|2020-01-01|7250C     |ABANILLA            |MURCIA               |174    |8,8 |0,0 |2,1 |07:44   |15,6|15:04   |NULL|NULL    |NULL |NULL     |83     |99   |08:20    |60   |14:00    |\n",
      "|2020-01-01|C453I     |CITFAGRO_88_GAITERO |STA. CRUZ DE TENERIFE|1744   |9,3 |0,0 |5,4 |05:20   |13,2|16:10   |NULL|NULL    |NULL |NULL     |NULL   |95   |14:20    |7    |08:20    |\n",
      "|2020-01-01|0255B     |SANTA SUSANNA       |BARCELONA            |40     |7,0 |0,0 |2,2 |02:50   |11,9|14:00   |21  |1,1     |6,4  |12:30    |80     |91   |Varias   |61   |12:50    |\n",
      "|2020-01-01|5612B     |LA RODA DE ANDALUCÍA|SEVILLA              |410    |11,8|0,0 |4,8 |08:00   |18,9|15:20   |18  |1,7     |6,4  |01:10    |70     |91   |Varias   |37   |16:00    |\n",
      "|2020-01-01|2885K     |FRESNO DE SAYAGO    |ZAMORA               |804    |0,4 |0,0 |-5,7|07:23   |6,5 |15:35   |06  |2,8     |5,8  |10:40    |92     |100  |Varias   |71   |15:30    |\n",
      "+----------+----------+--------------------+---------------------+-------+----+----+----+--------+----+--------+----+--------+-----+---------+-------+-----+---------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      " Datos con tipos convertidos:\n",
      "root\n",
      " |-- fecha: string (nullable = true)\n",
      " |-- indicativo: string (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- provincia: string (nullable = true)\n",
      " |-- altitud: string (nullable = true)\n",
      " |-- tmed: string (nullable = true)\n",
      " |-- prec: string (nullable = true)\n",
      " |-- tmin: string (nullable = true)\n",
      " |-- horatmin: string (nullable = true)\n",
      " |-- tmax: string (nullable = true)\n",
      " |-- horatmax: string (nullable = true)\n",
      " |-- dir: string (nullable = true)\n",
      " |-- velmedia: string (nullable = true)\n",
      " |-- racha: string (nullable = true)\n",
      " |-- horaracha: string (nullable = true)\n",
      " |-- hrMedia: string (nullable = true)\n",
      " |-- hrMax: string (nullable = true)\n",
      " |-- horaHrMax: string (nullable = true)\n",
      " |-- hrMin: string (nullable = true)\n",
      " |-- horaHrMin: string (nullable = true)\n",
      "\n",
      "+----------+----------+--------------------+---------------------+-------+----+----+----+--------+----+--------+----+--------+-----+---------+-------+-----+---------+-----+---------+\n",
      "|fecha     |indicativo|nombre              |provincia            |altitud|tmed|prec|tmin|horatmin|tmax|horatmax|dir |velmedia|racha|horaracha|hrMedia|hrMax|horaHrMax|hrMin|horaHrMin|\n",
      "+----------+----------+--------------------+---------------------+-------+----+----+----+--------+----+--------+----+--------+-----+---------+-------+-----+---------+-----+---------+\n",
      "|2020-01-01|7250C     |ABANILLA            |MURCIA               |174    |8,8 |0,0 |2,1 |07:44   |15,6|15:04   |NULL|NULL    |NULL |NULL     |83     |99   |08:20    |60   |14:00    |\n",
      "|2020-01-01|C453I     |CITFAGRO_88_GAITERO |STA. CRUZ DE TENERIFE|1744   |9,3 |0,0 |5,4 |05:20   |13,2|16:10   |NULL|NULL    |NULL |NULL     |NULL   |95   |14:20    |7    |08:20    |\n",
      "|2020-01-01|0255B     |SANTA SUSANNA       |BARCELONA            |40     |7,0 |0,0 |2,2 |02:50   |11,9|14:00   |21  |1,1     |6,4  |12:30    |80     |91   |Varias   |61   |12:50    |\n",
      "|2020-01-01|5612B     |LA RODA DE ANDALUCÍA|SEVILLA              |410    |11,8|0,0 |4,8 |08:00   |18,9|15:20   |18  |1,7     |6,4  |01:10    |70     |91   |Varias   |37   |16:00    |\n",
      "|2020-01-01|2885K     |FRESNO DE SAYAGO    |ZAMORA               |804    |0,4 |0,0 |-5,7|07:23   |6,5 |15:35   |06  |2,8     |5,8  |10:40    |92     |100  |Varias   |71   |15:30    |\n",
      "+----------+----------+--------------------+---------------------+-------+----+----+----+--------+----+--------+----+--------+-----+---------+-------+-----+---------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Guardando datos en Iceberg: landing_db.aemetRawDiario\n",
      "Archivo aemet_unificado_2020-01-01_2020-01-30.json eliminado correctamente.\n",
      "\n",
      "Ubicación de la tabla Iceberg:\n",
      "La tabla Iceberg se guardó en: ../data/warehouse/landing_db/aemetRawDiario\n",
      "\n",
      "- Schema de aemetRawDiario:\n",
      "+----------+---------+-------+\n",
      "|col_name  |data_type|comment|\n",
      "+----------+---------+-------+\n",
      "|fecha     |string   |NULL   |\n",
      "|indicativo|string   |NULL   |\n",
      "|nombre    |string   |NULL   |\n",
      "|provincia |string   |NULL   |\n",
      "|altitud   |string   |NULL   |\n",
      "|tmed      |string   |NULL   |\n",
      "|prec      |string   |NULL   |\n",
      "|tmin      |string   |NULL   |\n",
      "|horatmin  |string   |NULL   |\n",
      "|tmax      |string   |NULL   |\n",
      "|horatmax  |string   |NULL   |\n",
      "|dir       |string   |NULL   |\n",
      "|velmedia  |string   |NULL   |\n",
      "|racha     |string   |NULL   |\n",
      "|horaracha |string   |NULL   |\n",
      "|hrMedia   |string   |NULL   |\n",
      "|hrMax     |string   |NULL   |\n",
      "|horaHrMax |string   |NULL   |\n",
      "|hrMin     |string   |NULL   |\n",
      "|horaHrMin |string   |NULL   |\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import utils as utils\n",
    "import requests, time\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, explode\n",
    "\n",
    "def descargar_datos_aemet_raw(start_date, end_date, api_key, delay_seconds=0.5):\n",
    "    \"\"\"\n",
    "    Descarga todos los datos climáticos de AEMET y los guarda en un único JSON.\n",
    "    \n",
    "    Args:\n",
    "        start_date (datetime): Fecha de inicio\n",
    "        end_date (datetime): Fecha de fin\n",
    "        api_key (str): API key de AEMET\n",
    "        delay_seconds (float): Pausa entre llamadas a la API\n",
    "    \n",
    "    Returns:\n",
    "        str: Ruta del archivo JSON unificado guardado\n",
    "    \"\"\"\n",
    "    \n",
    "    # --------------------------------------------\n",
    "    # 1. Configuración inicial\n",
    "    # --------------------------------------------\n",
    "    headers = {'api_key': api_key}\n",
    "    base_url = 'https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/datos/fechaini/'\n",
    "    end_url = '/todasestaciones'\n",
    "    all_climatological_data = []\n",
    "    \n",
    "    print(f\" Iniciando descarga del período: {start_date.date()} a {end_date.date()}\")\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # 2. Generar intervalos de 15 días\n",
    "    # --------------------------------------------\n",
    "    def generar_intervalos(start, end):\n",
    "        delta = end - start\n",
    "        total_days = delta.days + 1\n",
    "        interval_days = 15\n",
    "        num_intervals = (total_days + interval_days - 1) // interval_days\n",
    "        \n",
    "        intervals = []\n",
    "        current_start = start\n",
    "        \n",
    "        for _ in range(num_intervals):\n",
    "            current_end = min(current_start + timedelta(days=interval_days-1), end)\n",
    "            intervals.append((\n",
    "                current_start.strftime('%Y-%m-%dT%H:%M:%SUTC'),\n",
    "                current_end.strftime('%Y-%m-%dT%H:%M:%SUTC')\n",
    "            ))\n",
    "            current_start = current_end + timedelta(days=1)\n",
    "        \n",
    "        return intervals\n",
    "    \n",
    "    date_intervals = generar_intervalos(start_date, end_date)\n",
    "    print(f\"Se procesarán {len(date_intervals)} intervalos de 15 días\")\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # 3. Descargar todos los datos\n",
    "    # --------------------------------------------\n",
    "    for i, (start_str, end_str) in enumerate(date_intervals, 1):\n",
    "        interval_url = f'{base_url}{start_str}/fechafin/{end_str}{end_url}'\n",
    "        print(f\"\\nProcesando intervalo {i}/{len(date_intervals)}: {start_str[:10]} a {end_str[:10]}\")\n",
    "\n",
    "        try:\n",
    "            # 3.1 Obtener URL de descarga\n",
    "            response_url = requests.get(interval_url, headers=headers)\n",
    "            time.sleep(delay_seconds * 0.3)\n",
    "            \n",
    "            if response_url.status_code != 200:\n",
    "                print(f\"Error en la solicitud (HTTP {response_url.status_code})\")\n",
    "                continue\n",
    "                \n",
    "            data_url = response_url.json().get('datos')\n",
    "            if not data_url:\n",
    "                print(\"No se encontró URL de datos en la respuesta\")\n",
    "                continue\n",
    "            \n",
    "            # 3.2 Descargar datos reales\n",
    "            response_data = requests.get(data_url)\n",
    "            if response_data.status_code == 200:\n",
    "                datos_intervalo = response_data.json()\n",
    "                all_climatological_data.extend(datos_intervalo)\n",
    "                print(f\"Descargados {len(datos_intervalo)} registros (Total acumulado: {len(all_climatological_data)})\")\n",
    "            else:\n",
    "                print(f\"Error al descargar datos (HTTP {response_data.status_code})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error en el intervalo: {str(e)}\")\n",
    "        \n",
    "        finally:\n",
    "            time.sleep(delay_seconds)\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # 4. Guardar todo en un único JSON\n",
    "    # --------------------------------------------\n",
    "    if not all_climatological_data:\n",
    "        print(\"\\nNo se descargaron datos válidos\")\n",
    "        return None\n",
    "    \n",
    "    archivojson = f\"aemet_unificado_{start_date.date()}_{end_date.date()}.json\"\n",
    "    \n",
    "    datos_completos = {\n",
    "        'metadata': {\n",
    "            'fecha_inicio': start_date.isoformat(),\n",
    "            'fecha_fin': end_date.isoformat(),\n",
    "            'fecha_generacion': datetime.now().isoformat(),\n",
    "            'total_registros': len(all_climatological_data),\n",
    "            'total_estaciones': len({d['indicativo'] for d in all_climatological_data})\n",
    "        },\n",
    "        'data': all_climatological_data\n",
    "    }\n",
    "    \n",
    "    with open(archivojson, 'w', encoding='utf-8') as f:\n",
    "        json.dump(datos_completos, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\n Archivo guardado como: {archivojson}\")\n",
    "    print(f\"Estadísticas:\")\n",
    "    print(f\"- Registros totales: {len(all_climatological_data)}\")\n",
    "    print(f\"- Estaciones únicas: {len({d['indicativo'] for d in all_climatological_data})}\")\n",
    "    \n",
    "    return archivojson\n",
    "\n",
    "pass    \n",
    "# --------------------------------------------\n",
    "# 1. Configura tu API Key\n",
    "# Es mejor no hardcodear la clave API directamente en el código para la seguridad.\n",
    "# Sin embargo, para este ejemplo, la asignaremos directamente.\n",
    "# En un proyecto real, usarías variables de entorno.\n",
    "API_KEY = \"eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJmcnZhcmdhcy44N0BnbWFpbC5jb20iLCJqdGkiOiI3MTJmNjFkYi1hMDg3LTRkM2QtODFlNS04ZjY4YjYwOWE2YTAiLCJpc3MiOiJBRU1FVCIsImlhdCI6MTc0OTIyOTY1OSwidXNlcklkIjoiNzEyZjYxZGItYTA4Ny00ZDNkLTgxZTUtOGY2OGI2MDlhNmEwIiwicm9sZSI6IiJ9.BbMqB0Jj2_z5wJw6luQhH7iMlJDMk2gfPEVOQ7Chc7E\"\n",
    "\n",
    "# 2. Define las fechas para la descarga\n",
    "# Aquí un ejemplo para descargar los datos de los últimos 30 días\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime(2020, 1, 30)\n",
    "\n",
    "# 3. Llama a la función con los argumentos\n",
    "# y maneja la creación del contexto Spark\n",
    "#if __name__ == \"__main__\":\n",
    "#try:\n",
    "        \n",
    "print(\"Iniciando la descarga de datos de AEMET...\")\n",
    "# La función devuelve el nombre del archivo generado\n",
    "nombre_archivo_generado = descargar_datos_aemet_raw(start_date, end_date, API_KEY)\n",
    "\n",
    "# Verificar que el archivo existe\n",
    "if not os.path.exists(nombre_archivo_generado):\n",
    "    raise FileNotFoundError(f\"No se encontró el archivo {nombre_archivo_generado}\")\n",
    "\n",
    "print(f\"\\nCargando archivo {nombre_archivo_generado} a Spark...\")\n",
    "\n",
    "# Configuracion Spark\n",
    "# 1. Crear la sesión de Spark correctamente\n",
    "spark = utils.create_context()\n",
    "# 2. Obtener el SparkContext desde la SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Ver el warehouse configurado (ubicación en caso de necesitarlo, se puede usar para verificacion)\n",
    "print(\"Warehouse configurado:\", spark.conf.get(\"spark.sql.catalog.spark_catalog.warehouse\"))\n",
    "# 3. Definir el esquema para el DataFrame\n",
    "esquema = StructType([\n",
    "    StructField(\"fecha\", StringType()),  # o StringType() si prefieres mantenerlo como texto\n",
    "    StructField(\"indicativo\", StringType()),\n",
    "    StructField(\"nombre\", StringType()),\n",
    "    StructField(\"provincia\", StringType()),\n",
    "    StructField(\"altitud\", StringType()),\n",
    "    StructField(\"tmed\", StringType()),      \n",
    "    StructField(\"prec\", StringType()),\n",
    "    StructField(\"tmin\", StringType()),\n",
    "    StructField(\"horatmin\", StringType()),\n",
    "    StructField(\"tmax\", StringType()),\n",
    "    StructField(\"horatmax\", StringType()),\n",
    "    StructField(\"dir\", StringType()),  # dirección del viento (grados)\n",
    "    StructField(\"velmedia\", StringType()),  # velocidad media del viento\n",
    "    StructField(\"racha\", StringType()),  # ráfaga máxima\n",
    "    StructField(\"horaracha\", StringType()),\n",
    "    StructField(\"hrMedia\", StringType()),  # humedad relativa media\n",
    "    StructField(\"hrMax\", StringType()),  # humedad relativa máxima\n",
    "    StructField(\"horaHrMax\", StringType()),\n",
    "    StructField(\"hrMin\", StringType()),  # humedad relativa mínima\n",
    "    StructField(\"horaHrMin\", StringType())\n",
    "])\n",
    "# 4. Cargar json y convertir a DataFrame\n",
    "#rdd = sc.wholeTextFiles(nombre_archivo_generado)\n",
    "df_raw = spark.read.option(\"multiline\", \"true\").json(nombre_archivo_generado)\n",
    "\n",
    "# 5. Procesar Json con Explode\n",
    "# Usar explode para descomponer el JSON y forzando al squema definido\n",
    "print(\"\\n Procesando JSON a Spark...\")\n",
    "df_spark_aemet = df_raw.select(explode(col(\"data\")).alias(\"row\")) \\\n",
    "    .selectExpr(\"row.*\") \\\n",
    "    .selectExpr(\"*\") \\\n",
    "    .selectExpr(*[f\"CAST({c} AS STRING)\" for c in esquema.fieldNames()])\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\n Datos cargados correctamente...\")\n",
    "print(f\"\\nTotal registros: {df_spark_aemet.count()}\")\n",
    "df_spark_aemet.show(5,truncate=False)\n",
    "\n",
    "#mostrar datos con tipos convertidos\n",
    "print(\"\\n Datos con tipos convertidos:\")\n",
    "df_spark_aemet.printSchema()\n",
    "df_spark_aemet.show(5, truncate=False)\n",
    "#         \n",
    "# 6. Guardar en Iceberg        \n",
    "# 6.1 Definir nombres de base de datos y tabla\n",
    "db_name = \"landing_db\"\n",
    "table_name = \"aemetRawDiario\"\n",
    "# 6.2 Guardar en Iceberg (usando función utils)\n",
    "print(f\"\\nGuardando datos en Iceberg: {db_name}.{table_name}\")\n",
    "utils.create_iceberg_table(spark, df_spark_aemet, db_name, table_name)\n",
    "# Eliminar el archivo JSON generado\n",
    "if os.path.exists(nombre_archivo_generado):\n",
    "    os.remove(nombre_archivo_generado)\n",
    "    print(f\"Archivo {nombre_archivo_generado} eliminado correctamente.\")\n",
    "else:\n",
    "    print(f\"Archivo {nombre_archivo_generado} no encontrado para eliminar.\")\n",
    "\n",
    "# 6.3 Verificar ubicación de la tabla Iceberg\n",
    "print(\"\\nUbicación de la tabla Iceberg:\")\n",
    "#spark.sql(\"DESCRIBE FORMATTED local_db.aemetRawDiario\").filter(\"col_name = 'Location'\").show(truncate=False)\n",
    "location = (\n",
    "spark.sql(f\"DESCRIBE FORMATTED {db_name}.{table_name}\")\n",
    "    .filter(\"col_name = 'Location'\")\n",
    "    .select(\"data_type\")\n",
    "    .collect()[0][0]\n",
    ")\n",
    "print(f\"La tabla Iceberg se guardó en: {location}\")\n",
    "\n",
    "# Verificación\n",
    "#print(\"\\nVerificación:\")\n",
    "#print(f\"- Tablas en {db_name}:\")\n",
    "#spark.sql(f\"SHOW TABLES IN {db_name}\").show(truncate=False)\n",
    "#listar tablas\n",
    "#print(f\"\\n- Tablas en {db_name}:\", [s.name for s in spark.catalog.listTables(db_name)])\n",
    "#print(f\"\\n- Tabla {table_name} existe:\", spark.catalog.tableExists(f\"{db_name}.{table_name}\"))\n",
    "#print(f\"- Cantidad de registros en {table_name}: {spark.table(f'{db_name}.{table_name}').count()}\")\n",
    "\n",
    "\n",
    "print(f\"\\n- Schema de {table_name}:\")\n",
    "spark.sql(f\"DESCRIBE {db_name}.{table_name}\").show(truncate=False)\n",
    "\n",
    "#except Exception as e:\n",
    "#    print(f\"\\n Error: {str(e)}\")\n",
    "#raise e\n",
    "#finally:\n",
    "# Limpieza con tiempo para evitar warnings\n",
    "#time.sleep(3)\n",
    "#if 'spark' in locals():\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TravelMind-D0cwe-Yj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
