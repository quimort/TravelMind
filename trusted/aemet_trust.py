from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import col, regexp_replace
from pyspark.sql.types import DoubleType, IntegerType
import utils as utils
import sys
import os
def check_spark_alive(spark):
    try:
        _ = spark.version  # Fuerza acceso a la sesi√≥n
        return True
    except Exception as e:
        print(f"‚ùå Spark no est√° disponible: {e}")
        return False

# --------------------------
# Funci√≥n para reemplazar comas por puntos en columnas string num√©ricas
# --------------------------
def replace_commas_with_dots(df, columns):
    """
    Reemplaza ',' por '.' en columnas especificadas y convierte a DoubleType.
    """
    for c in columns:
        if c in df.columns:
            df = df.withColumn(c, regexp_replace(col(c), ",", ".").cast(DoubleType()))
    return df

if __name__ == "__main__":
    # 1Ô∏è‚É£ Crear sesi√≥n Spark
    spark = utils.create_context_trusted()  # üëà usa el nuevo contexto con cat√°logo propio y rutas cortas

    # Tablas (ojo al cat√°logo)
    # Tablas (ojo al cat√°logo)
    # RAW: si tu RAW est√° en otro cat√°logo, acc√©delo con su nombre completo (aj√∫stalo a tu caso real).
    # Ejemplo si el RAW est√° en spark_catalog: spark.table("local_db.aemetRawDiario")
    # o si tambi√©n est√° en trustedcat: spark.table("trustedcat.local_db.aemetRawDiario")
    print("üì• Leyendo tabla RAW: spark_catalog.local_db.aemetRawDiario")
    # 1Ô∏è‚É£ Crear la DB en el cat√°logo spark_catalog si no existe
    spark.sql("CREATE DATABASE IF NOT EXISTS spark_catalog.local_db")

    spark.sql("""
    CREATE TABLE IF NOT EXISTS spark_catalog.local_db.aemetRawDiario
    USING ICEBERG
    LOCATION './data/warehouse/local_db/aemetRawDiario'
    """)

    df_raw = spark.read.table("spark_catalog.local_db.aemetRawDiario")  # si es spark_catalog por defecto
    # Si no, usa: df_raw = spark.table("trustedcat.local_db.aemetRawDiario")
    print(f"üìä Registros RAW: {df_raw.count()}")
    #confirmar que la tabla existe
    spark.sql("SHOW DATABASES IN spark_catalog").show()
    spark.sql("SHOW TABLES IN spark_catalog.local_db").show()

    # numeric_cols = ["tmed","prec","tmin","tmax","velmedia","racha","hrMedia","hrMax","hrMin","altitud"]
    # print("üîÑ Normalizando decimales y casteando a Double...")
    # df_clean = replace_commas_with_dots(df_raw, numeric_cols)

    # # Test previo con subset en Parquet (ruta corta)
    # TEST_PATH = r"C:\parq_test_trusted"
    # os.makedirs(TEST_PATH, exist_ok=True)
    # print(f"üìù Test Parquet (1000 filas) en: {TEST_PATH}")
    # df_sample = df_clean.limit(1000)
    # df_sample.repartition(4).write.mode("overwrite").parquet(TEST_PATH)
    # print("‚úÖ Test Parquet OK")

    # # Repartici√≥n segura antes de escribir Iceberg
    # df_trusted = df_clean.repartition(8)

    # # Escribe la tabla Iceberg en el cat√°logo 'trustedcat'
    # db_name = "trusted"
    # tbl_name = "aemetTrustedDiario"
    # full_tbl = f"trustedcat.{db_name}.{tbl_name}"

    # print(f"üíæ Sobrescribiendo tabla Iceberg: {full_tbl}")
    # # Opci√≥n 1: DataFrameWriter V2 (recomendado)
    # (
    #     df_trusted.writeTo(full_tbl)
    #     .option("overwrite-mode", "dynamic")
    #     .createOrReplace()
    # )
    # print("‚úÖ Tabla Iceberg guardada correctamente")

    spark.stop()
    # print("üèÅ Proceso completado")