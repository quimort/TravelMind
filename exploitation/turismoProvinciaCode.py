from pyspark.sql import SparkSession
from pyspark.sql.functions import upper, col
import os
import utils
import pyspark
from pyspark.sql import SparkSession,DataFrame
import requests
import json 
from io import BytesIO
import pandas as pd
import os
import sys
import utils as utils


# 1) Re-use the same Iceberg-aware session for read & write
spark = utils.create_context()


# Read landing-zone Iceberg table
src_db, src_tbl = "trusted", "turismo_Provincia"
print(f"→ Reading spark_catalog.{src_db}.{src_tbl}")
df = utils.read_iceberg_table(spark, src_db, src_tbl)
df.show()
# 5) Clean & normalize
print("→ Filtering for PROVINCIA_DESTINO = 'Barcelona'")
df_barcelona = df.filter(col("PROVINCIA_DESTINO") == "BARCELONA")
# Write into exploitation zone
df_barcelona.show()
tgt_db, tgt_tbl = "exploitation", "turismo_Provincia"
print(f"→ Writing spark_catalog.{tgt_db}.{tgt_tbl}")
utils.overwrite_iceberg_table(spark, df_barcelona, tgt_db, tgt_tbl)

print("✅ Exploitation load complete.")


spark.stop()