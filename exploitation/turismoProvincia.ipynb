{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4b13b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Reading trusted CSV from c:\\Users\\joaqu\\OneDrive\\Documents\\AAmaster_UPC\\TFM\\TravelMind\\POC\\trusted\\data\\trusted\\turismo_Provincia_clean_csv\n",
      "→ Filtering for PROVINCIA_DESTINO = 'Barcelona'\n",
      "→ Writing exploitation CSV to c:\\Users\\joaqu\\OneDrive\\Documents\\AAmaster_UPC\\TFM\\TravelMind\\POC\\exploitation\\data\\exploitation\\turismo_Provincia_barcelona\n",
      "✅ Exploitation CSV load complete.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 0) Pin the Python interpreter for Spark\n",
    "os.environ[\"PYSPARK_PYTHON\"]        = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "def main():\n",
    "    # 1) Determine POC root (one level up from exploitation/)\n",
    "    cwd      = os.getcwd()\n",
    "    poc_root = os.path.abspath(os.path.join(cwd, \"..\"))\n",
    "\n",
    "    # 2) Path to trusted CSV (from your previous step)\n",
    "    trusted_dir = os.path.join(\n",
    "        poc_root, \"trusted\", \"data\", \"trusted\", \"turismo_Provincia_clean_csv\"\n",
    "    )\n",
    "    print(\"→ Reading trusted CSV from\", trusted_dir)\n",
    "\n",
    "    # 3) Start Spark\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "          .appName(\"ExploitationZoneLoad\")\n",
    "          .config(\"spark.master\", \"local[*]\")\n",
    "          .config(\"spark.driver.memory\", \"2g\")\n",
    "          .getOrCreate()\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # 4) Load the trusted CSV\n",
    "        if not os.path.isdir(trusted_dir):\n",
    "            raise FileNotFoundError(f\"Trusted folder not found: {trusted_dir}\")\n",
    "        df = (\n",
    "            spark.read\n",
    "                 .option(\"header\", True)\n",
    "                 .csv(trusted_dir)\n",
    "        )\n",
    "\n",
    "        # 5) Filter for Barcelona\n",
    "        print(\"→ Filtering for PROVINCIA_DESTINO = 'Barcelona'\")\n",
    "        df_barcelona = df.filter(col(\"PROVINCIA_DESTINO\") == \"Barcelona\")\n",
    "\n",
    "        # 6) Write out exploitation CSV\n",
    "        exploit_dir = os.path.join(\n",
    "            poc_root, \"exploitation\", \"data\", \"exploitation\", \"turismo_Provincia_barcelona\"\n",
    "        )\n",
    "        os.makedirs(exploit_dir, exist_ok=True)\n",
    "        print(\"→ Writing exploitation CSV to\", exploit_dir)\n",
    "\n",
    "        (\n",
    "            df_barcelona\n",
    "              .repartition(1)\n",
    "              .write\n",
    "              .mode(\"overwrite\")\n",
    "              .option(\"header\", True)\n",
    "              .csv(exploit_dir)\n",
    "        )\n",
    "\n",
    "        print(\"✅ Exploitation CSV load complete.\")\n",
    "\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4b05615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import upper, col\n",
    "import os\n",
    "import utils\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession,DataFrame\n",
    "import requests\n",
    "import json \n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcda4c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Re-use the same Iceberg-aware session for read & write\n",
    "spark = utils.create_context()\n",
    "\n",
    "\n",
    "# Read landing-zone Iceberg table\n",
    "src_db, src_tbl = \"trusted\", \"turismo_Provincia\"\n",
    "print(f\"→ Reading spark_catalog.{src_db}.{src_tbl}\")\n",
    "df = utils.read_iceberg_table(spark, src_db, src_tbl)\n",
    "# 5) Clean & normalize\n",
    "print(\"→ Filtering for PROVINCIA_DESTINO = 'Barcelona'\")\n",
    "df_barcelona = df.filter(col(\"PROVINCIA_DESTINO\") == \"Barcelona\")\n",
    "# Write into exploitation zone\n",
    "tgt_db, tgt_tbl = \"exploitation\", \"turismo_Provincia\"\n",
    "print(f\"→ Writing spark_catalog.{tgt_db}.{tgt_tbl}\")\n",
    "utils.overwrite_iceberg_table(spark, df_barcelona, tgt_db, tgt_tbl)\n",
    "\n",
    "print(\"✅ Exploitation load complete.\")\n",
    "\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TravelMind--WcSvJCM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
