{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae6da74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "import utils as utils\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from builtins import min as python_min\n",
    "import mlflow\n",
    "import mlflow.spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe9e7008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/27 18:53:19 INFO mlflow.tracking.fluent: Experiment with name 'visitas-ciudad-model' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///d:/Quim/Documents/quim%20documents/Master/TFM/TravelMind/ml_models/mlruns/279040930689046531', creation_time=1756313599561, experiment_id='279040930689046531', last_update_time=1756313599561, lifecycle_stage='active', name='visitas-ciudad-model', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"visitas-ciudad-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bbf44c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = utils.create_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb515de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loading hotel occupancy data...\n",
      "    Hotel records: 265\n",
      "✅ Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load weather data\n",
    "#print(\"  Loading weather data...\")\n",
    "#df_weather = utils.read_iceberg_table( spark, \"trusted\", \"aemetTrustedDiario\")\n",
    "#\n",
    "#weather_count = df_weather.count()\n",
    "#print(f\"    Weather records: {weather_count}\")\n",
    "\n",
    "# Load hotel occupancy data\n",
    "print(\"=== Loading source tables ===\")\n",
    "\n",
    "# ------------------------------\n",
    "# Hoteles\n",
    "print(\"  Loading hotel occupancy data...\")\n",
    "df_hotels = utils.read_iceberg_table(\n",
    "    spark=spark, \n",
    "    db_name=\"exploitation\", \n",
    "    table_name=\"f_ocupacion_barcelona\"\n",
    ")\n",
    "print(f\"    Hotel records: {df_hotels.count()}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Apartamentos turísticos\n",
    "print(\"  Loading apartamentos data...\")\n",
    "df_apartamentos = utils.read_iceberg_table(\n",
    "    spark=spark,\n",
    "    db_name=\"landing\",\n",
    "    table_name=\"apartamentos_turisticos\"\n",
    ")\n",
    "print(f\"    Apartamentos records: {df_apartamentos.count()}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Actividades de ocio\n",
    "print(\"  Loading actividades ocio data...\")\n",
    "df_ocio = utils.read_iceberg_table(\n",
    "    spark=spark,\n",
    "    db_name=\"landing\",\n",
    "    table_name=\"actividades_ocio\"\n",
    ")\n",
    "print(f\"    Ocio records: {df_ocio.count()}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Calidad del aire\n",
    "print(\"  Loading calidad aire data...\")\n",
    "df_calidad = utils.read_iceberg_table(\n",
    "    spark=spark,\n",
    "    db_name=\"landing\",\n",
    "    table_name=\"calidad_aire\"\n",
    ")\n",
    "print(f\"    Calidad aire records: {df_calidad.count()}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Tráfico semanal\n",
    "print(\"  Loading trafico semanal data...\")\n",
    "df_trafico = utils.read_iceberg_table(\n",
    "    spark=spark,\n",
    "    db_name=\"landing\",\n",
    "    table_name=\"trafico_semana\"\n",
    ")\n",
    "print(f\"    Trafico records: {df_trafico.count()}\")\n",
    "\n",
    "print(\"✅ Data loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7e610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hotel_features(df_hotels):\n",
    "        \"\"\"Create hotel-based features.\"\"\"\n",
    "        print(\"  Creating hotel features...\")\n",
    "        \n",
    "        df_hotel_features = df_hotels.groupBy(\n",
    "            col('año'),\n",
    "            col('mes')\n",
    "        ).agg(\n",
    "            sum('viajeros').alias('hotel_viajeros'),\n",
    "            sum('pernoctaciones').alias('hotel_pernoctaciones'),\n",
    "            avg('estanciaMedia').alias('hotel_estancia_media'),\n",
    "            avg('gradoOcupaPlazas').alias('avg_ocupacion')\n",
    "        ).withColumn(\n",
    "            # Hotel availability score\n",
    "            'hotel_availability_score',\n",
    "            100 - col('avg_ocupacion')\n",
    "        )\n",
    "        \n",
    "        return df_hotel_features\n",
    "\n",
    "def create_apartment_features(df_apartments):\n",
    "    \"\"\"Create apartment-based features.\"\"\"\n",
    "    print(\"  Creating apartment features...\")\n",
    "    \n",
    "    df_apartment_features = df_apartments.groupBy(\n",
    "        col(\"AÑO\"),\n",
    "        col(\"MES\")\n",
    "    ).agg(\n",
    "        sum(\"VIAJEROS\").alias(\"apt_viajeros\"),\n",
    "        sum(\"PERNOCTACIONES\").alias(\"apt_pernoctaciones\"),\n",
    "        avg(\"ESTANCIA_MEDIA\").alias(\"apt_estancia_media\"),\n",
    "        avg(\"GRADO_OCUPA_PLAZAS\").alias(\"avg_ocupa_plazas\"),\n",
    "        avg(\"GRADO_OCUPA_APART\").alias(\"avg_ocupa_apart\"),\n",
    "        avg(\"GRADO_OCUPA_APART_FIN_SEMANA\").alias(\"avg_ocupa_apart_weekend\"),\n",
    "        sum(\"APARTAMENTOS_ESTIMADOS\").alias(\"apt_estimados\"),\n",
    "        sum(\"PLAZAS_ESTIMADAS\").alias(\"plazas_estimadas\"),\n",
    "        sum(\"PERSONAL_EMPLEADO\").alias(\"apt_personal_empleado\")\n",
    "    ).withColumn(\n",
    "        # Apartment availability score (basado en ocupación de plazas)\n",
    "        \"apt_availability_score\",\n",
    "        100 - col(\"avg_ocupa_plazas\")\n",
    "    )\n",
    "    \n",
    "    return df_apartment_features\n",
    "\n",
    "def create_leisure_features(df_leisure):\n",
    "    \"\"\"Create leisure-based features from actividades_ocio table.\"\"\"\n",
    "    print(\"  Creating leisure features...\")\n",
    "\n",
    "    df_leisure_features = df_leisure.groupBy(\n",
    "        col(\"AÑO\"),\n",
    "        col(\"MES\")\n",
    "    ).agg(\n",
    "        sum(\"ENTRADAS\").alias(\"ocio_total_entradas\"),\n",
    "        sum(\"VISITAS_PAGINAS\").alias(\"ocio_total_visitas_paginas\"),\n",
    "        sum(\"GASTO_TOTAL\").alias(\"ocio_gasto_total\"),\n",
    "        avg(\"PRECIO_MEDIO_ENTRADA\").alias(\"ocio_precio_medio_entrada\"),\n",
    "        sum(\"TRANSACCIONES\").alias(\"ocio_total_transacciones\")\n",
    "    ).withColumn(\n",
    "        # Engagement score: visitas / transacciones (más alto = más interés online por compra)\n",
    "        \"ocio_engagement_score\",\n",
    "        (col(\"ocio_total_visitas_paginas\") / (col(\"ocio_total_transacciones\") + 1))\n",
    "    ).withColumn(\n",
    "        # Gasto medio por entrada\n",
    "        \"ocio_gasto_medio_por_entrada\",\n",
    "        (col(\"ocio_gasto_total\") / (col(\"ocio_total_entradas\") + 1))\n",
    "    )\n",
    "\n",
    "    return df_leisure_features\n",
    "\n",
    "def create_air_quality_features(df_air):\n",
    "    \"\"\"Create air quality features from calidad_aire table.\"\"\"\n",
    "    print(\"  Creating air quality features...\")\n",
    "\n",
    "    df_air_features = df_air.groupBy(\n",
    "        col(\"AÑO\"),\n",
    "        col(\"MES\")\n",
    "    ).agg(\n",
    "        # Porcentaje medio del mes de calidad de aire buena\n",
    "        avg(when(col(\"CALIDAD_AIRE\") == \"Buena\", col(\"PORCENTAJE_CALIDAD_AIRE\"))).alias(\"aire_pct_buena\"),\n",
    "        # Porcentaje medio del mes de calidad de aire aceptable\n",
    "        avg(when(col(\"CALIDAD_AIRE\") == \"Aceptable\", col(\"PORCENTAJE_CALIDAD_AIRE\"))).alias(\"aire_pct_aceptable\"),\n",
    "        # Porcentaje medio del mes de calidad de aire mala\n",
    "        avg(when(col(\"CALIDAD_AIRE\") == \"Mala\", col(\"PORCENTAJE_CALIDAD_AIRE\"))).alias(\"aire_pct_mala\"),\n",
    "        # Número de estaciones monitorizadas\n",
    "        countDistinct(\"ESTACION\").alias(\"aire_num_estaciones\")\n",
    "    ).withColumn(\n",
    "        # Índice simplificado: pondera calidad (buena=2, aceptable=1, mala=0)\n",
    "        \"aire_quality_index\",\n",
    "        (\n",
    "            col(\"aire_pct_buena\") * 2 +\n",
    "            col(\"aire_pct_aceptable\") * 1 +\n",
    "            col(\"aire_pct_mala\") * 0\n",
    "        ) / (col(\"aire_pct_buena\") + col(\"aire_pct_aceptable\") + col(\"aire_pct_mala\") + 1e-6)\n",
    "    )\n",
    "\n",
    "    return df_air_features\n",
    "\n",
    "def create_trafico_features(df_trafico):\n",
    "    \"\"\"Create traffic features from trafico_semana table.\"\"\"\n",
    "    print(\"  Creating traffic features...\")\n",
    "\n",
    "    df_trafico_features = df_trafico.groupBy(\n",
    "        col(\"AÑO\"),\n",
    "        col(\"MES\")\n",
    "    ).agg(\n",
    "        # Intensidad media de vehículos ligeros en el mes\n",
    "        avg(\"IMD_VEHICULO_LIGERO\").alias(\"trafico_imd_ligeros\"),\n",
    "        # Intensidad media de vehículos pesados en el mes\n",
    "        avg(\"IMD_VEHICULO_PESADO\").alias(\"trafico_imd_pesados\"),\n",
    "        # Intensidad media total\n",
    "        avg(\"IMD_VEHICULO_TOTAL\").alias(\"trafico_imd_total\"),\n",
    "        # Total mensual de vehículos (ligeros + pesados)\n",
    "        sum(\"IMD_VEHICULO_TOTAL\").alias(\"trafico_total_mes\"),\n",
    "        # Número de estaciones activas\n",
    "        countDistinct(\"ESTACION\").alias(\"trafico_num_estaciones\")\n",
    "    ).withColumn(\n",
    "        # Ratio de pesados sobre total (indicador económico-logístico)\n",
    "        \"trafico_pct_pesados\",\n",
    "        col(\"trafico_imd_pesados\") / (col(\"trafico_imd_total\") + 1e-6)\n",
    "    )\n",
    "\n",
    "    return df_trafico_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f2bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Creating hotel features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 18:53:42,890 INFO XGBoost-PySpark: _fit Running xgboost-3.0.4 on 1 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'device': 'cpu', 'max_depth': 5, 'scale_pos_weight': 0.9506172839506173, 'num_round': 50, 'eta': 0.1, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-08-27 18:53:51,277 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2025/08/27 18:54:09 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\Quim\\AppData\\Local\\Temp\\tmp2u8yo4yx\\model, flavor: spark). Fall back to return ['pyspark==3.5.6']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/08/27 18:54:09 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo registrado en MLflow con run_id: 467a75a48f5a4cf48f7b4627a9daf6df\n"
     ]
    }
   ],
   "source": [
    "#2. Generar features a partir de df_hotels\n",
    "print(\"=== Generating features ===\")\n",
    "\n",
    "# Llamar a las funciones de features\n",
    "df_hotels_features       = create_hotel_features(df_hotels)\n",
    "df_apartamentos_features = create_apartamentos_features(df_apartamentos)\n",
    "df_ocio_features         = create_ocio_features(df_ocio)\n",
    "df_calidad_features      = create_calidad_aire_features(df_calidad)\n",
    "df_trafico_features      = create_trafico_features(df_trafico)\n",
    "\n",
    "print(\"=== Joining all feature sets ===\")\n",
    "\n",
    "# Unir por AÑO y MES (base temporal común)\n",
    "df_features = df_hotels_features \\\n",
    "    .join(df_apartamentos_features, [\"AÑO\", \"MES\"], \"outer\") \\\n",
    "    .join(df_ocio_features, [\"AÑO\", \"MES\"], \"outer\") \\\n",
    "    .join(df_calidad_features, [\"AÑO\", \"MES\"], \"outer\") \\\n",
    "    .join(df_trafico_features, [\"AÑO\", \"MES\"], \"outer\")\n",
    "\n",
    "print(f\"Final feature dataset has {df_features.count()} rows\")\n",
    "\n",
    "# 🚨 Aquí necesitas una etiqueta (label).\n",
    "# Ejemplo: 1 si es \"buen momento\", 0 si no. \n",
    "# Esto normalmente lo defines con criterios propios.\n",
    "# Supongamos que un buen momento es cuando hotel_availability_score > 40\n",
    "df_labeled = df_hotel_features.withColumn(\n",
    "    \"label\", (col(\"hotel_availability_score\") > 40).cast(\"int\")\n",
    ")\n",
    "\n",
    "# 3. Seleccionar las features para el modelo\n",
    "feature_cols = [\n",
    "    \"hotel_viajeros\",\n",
    "    \"hotel_pernoctaciones\",\n",
    "    \"hotel_estancia_media\",\n",
    "    \"avg_ocupacion\",\n",
    "    \"hotel_availability_score\"\n",
    "]\n",
    "\n",
    "# VectorAssembler para convertir a vector de features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "# Contar positivos (label=1) y negativos (label=0)\n",
    "counts = df_labeled.groupBy(\"label\").count().collect()\n",
    "\n",
    "# Inicializamos variables\n",
    "num_positivos = 0\n",
    "num_negativos = 0\n",
    "\n",
    "for row in counts:\n",
    "    if row['label'] == 1:\n",
    "        num_positivos = row['count']\n",
    "    else:\n",
    "        num_negativos = row['count']\n",
    "\n",
    "# Ratio para scale_pos_weight\n",
    "ratio_negativos_sobre_positivos = num_negativos / num_positivos\n",
    "# 4. Definir modelo XGBoost\n",
    "xgb = SparkXGBClassifier(\n",
    "    features_col=\"features\",       # <- snake_case\n",
    "    label_col=\"label\",\n",
    "    prediction_col=\"prediction\",\n",
    "    probability_col=\"probability\",\n",
    "    num_round=50,\n",
    "    max_depth=5,\n",
    "    eta=0.1,\n",
    "    scale_pos_weight=ratio_negativos_sobre_positivos\n",
    ")\n",
    "\n",
    "# 5. Construir pipeline\n",
    "pipeline = Pipeline(stages=[assembler, xgb])\n",
    "\n",
    "# 6. Entrenar modelo\n",
    "train_df, test_df = df_labeled.randomSplit([0.8, 0.2], seed=42)\n",
    "#model = pipeline.fit(train)\n",
    "#\n",
    "## 7. Evaluar modelo\n",
    "#predictions = model.transform(test)\n",
    "#predictions.select(\"año\", \"mes\", \"label\", \"probability\", \"prediction\").show(10, truncate=False)\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    # Entrenar\n",
    "    model = pipeline.fit(train_df)\n",
    "    preds = model.transform(test_df)\n",
    "\n",
    "    # Evaluación\n",
    "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    "    )\n",
    "    acc = evaluator.evaluate(preds)\n",
    "\n",
    "    # Log de parámetros, métricas y modelo\n",
    "    mlflow.log_param(\"max_depth\", 5)\n",
    "    mlflow.log_param(\"eta\", 0.1)\n",
    "    mlflow.log_param(\"num_round\", 50)\n",
    "    mlflow.log_param(\"scale_pos_weight\", ratio_negativos_sobre_positivos)\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "\n",
    "    # Loggear modelo Spark\n",
    "    mlflow.spark.log_model(model, \"spark_xgb_model\")\n",
    "\n",
    "    print(\"Modelo registrado en MLflow con run_id:\", run.info.run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c70287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/27 19:01:29 INFO mlflow.spark: File 'd:/Quim/Documents/quim documents/Master/TFM/TravelMind/ml_models/mlruns/279040930689046531/467a75a48f5a4cf48f7b4627a9daf6df/artifacts/spark_xgb_model/sparkml' is already on DFS, copy is not necessary.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o800.partitions.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(Ljava/lang/String;)Lorg/apache/hadoop/io/nativeio/NativeIO$POSIX$Stat;\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.getStat(NativeIO.java:608)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfoByNativeIO(RawLocalFileSystem.java:934)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:848)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:816)\r\n\tat org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:52)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2199)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2179)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m future_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame([future_row])\n\u001b[0;32m     34\u001b[0m model_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/Quim/Documents/quim documents/Master/TFM/TravelMind/ml_models/mlruns/279040930689046531/467a75a48f5a4cf48f7b4627a9daf6df/artifacts/spark_xgb_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 35\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Aplicar pipeline entrenado\u001b[39;00m\n\u001b[0;32m     37\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(future_df)\n",
      "File \u001b[1;32mc:\\Users\\Quim\\.virtualenvs\\TravelMind-69GU69Sq\\lib\\site-packages\\mlflow\\spark\\__init__.py:961\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(model_uri, dfs_tmpdir, dst_path)\u001b[0m\n\u001b[0;32m    959\u001b[0m sparkml_model_uri \u001b[38;5;241m=\u001b[39m append_to_uri_path(model_uri, flavor_conf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_data\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    960\u001b[0m local_sparkml_model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(local_mlflow_model_path, flavor_conf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_data\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 961\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparkml_model_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdfs_tmpdir_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdfs_tmpdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_sparkml_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Quim\\.virtualenvs\\TravelMind-69GU69Sq\\lib\\site-packages\\mlflow\\spark\\__init__.py:885\u001b[0m, in \u001b[0;36m_load_model\u001b[1;34m(model_uri, dfs_tmpdir_base, local_model_path)\u001b[0m\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_model_databricks_dbfs(\n\u001b[0;32m    882\u001b[0m         dfs_tmpdir, local_model_path \u001b[38;5;129;01mor\u001b[39;00m _download_artifact_from_uri(model_uri)\n\u001b[0;32m    883\u001b[0m     )\n\u001b[0;32m    884\u001b[0m model_uri \u001b[38;5;241m=\u001b[39m _HadoopFileSystem\u001b[38;5;241m.\u001b[39mmaybe_copy_from_uri(model_uri, dfs_tmpdir, local_model_path)\n\u001b[1;32m--> 885\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPipelineModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Quim\\.virtualenvs\\TravelMind-69GU69Sq\\lib\\site-packages\\pyspark\\ml\\util.py:369\u001b[0m, in \u001b[0;36mMLReadable.load\u001b[1;34m(cls, path)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RL:\n\u001b[0;32m    368\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Quim\\.virtualenvs\\TravelMind-69GU69Sq\\lib\\site-packages\\pyspark\\ml\\pipeline.py:282\u001b[0m, in \u001b[0;36mPipelineModelReader.load\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipelineModel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 282\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mDefaultParamsReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadMetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m JavaMLReader(cast(Type[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJavaMLReadable[PipelineModel]\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls))\u001b[38;5;241m.\u001b[39mload(path)\n",
      "File \u001b[1;32mc:\\Users\\Quim\\.virtualenvs\\TravelMind-69GU69Sq\\lib\\site-packages\\pyspark\\ml\\util.py:579\u001b[0m, in \u001b[0;36mDefaultParamsReader.loadMetadata\u001b[1;34m(path, sc, expectedClassName)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03mLoad metadata saved using :py:meth:`DefaultParamsWriter.saveMetadata`\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;124;03m    If non empty, this is checked against the loaded metadata.\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    578\u001b[0m metadataPath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 579\u001b[0m metadataStr \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadataPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    580\u001b[0m loadedVals \u001b[38;5;241m=\u001b[39m DefaultParamsReader\u001b[38;5;241m.\u001b[39m_parseMetaData(metadataStr, expectedClassName)\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loadedVals\n",
      "File \u001b[1;32mc:\\Users\\Quim\\.virtualenvs\\TravelMind-69GU69Sq\\lib\\site-packages\\pyspark\\rdd.py:2888\u001b[0m, in \u001b[0;36mRDD.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2862\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   2863\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2864\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[0;32m   2865\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2886\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[0;32m   2887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2888\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2889\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[0;32m   2890\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Quim\\.virtualenvs\\TravelMind-69GU69Sq\\lib\\site-packages\\pyspark\\rdd.py:2822\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   2781\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2782\u001b[0m \u001b[38;5;124;03mTake the first num elements of the RDD.\u001b[39;00m\n\u001b[0;32m   2783\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2819\u001b[0m \u001b[38;5;124;03m[91, 92, 93]\u001b[39;00m\n\u001b[0;32m   2820\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2821\u001b[0m items: List[T] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 2822\u001b[0m totalParts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetNumPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2823\u001b[0m partsScanned \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   2825\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(items) \u001b[38;5;241m<\u001b[39m num \u001b[38;5;129;01mand\u001b[39;00m partsScanned \u001b[38;5;241m<\u001b[39m totalParts:\n\u001b[0;32m   2826\u001b[0m     \u001b[38;5;66;03m# The number of partitions to try in this iteration.\u001b[39;00m\n\u001b[0;32m   2827\u001b[0m     \u001b[38;5;66;03m# It is ok for this number to be greater than totalParts because\u001b[39;00m\n\u001b[0;32m   2828\u001b[0m     \u001b[38;5;66;03m# we actually cap it at totalParts in runJob.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Quim\\.virtualenvs\\TravelMind-69GU69Sq\\lib\\site-packages\\pyspark\\rdd.py:952\u001b[0m, in \u001b[0;36mRDD.getNumPartitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgetNumPartitions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    936\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    937\u001b[0m \u001b[38;5;124;03m    Returns the number of partitions in RDD\u001b[39;00m\n\u001b[0;32m    938\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;124;03m    2\u001b[39;00m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 952\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[1;32mc:\\Users\\Quim\\.virtualenvs\\TravelMind-69GU69Sq\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Quim\\.virtualenvs\\TravelMind-69GU69Sq\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Quim\\.virtualenvs\\TravelMind-69GU69Sq\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o800.partitions.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(Ljava/lang/String;)Lorg/apache/hadoop/io/nativeio/NativeIO$POSIX$Stat;\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.getStat(NativeIO.java:608)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfoByNativeIO(RawLocalFileSystem.java:934)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:848)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:816)\r\n\tat org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:52)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2199)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2179)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Supongamos que queremos predecir para Barcelona, 20/09/2025\n",
    "ciudad = \"Barcelona\"\n",
    "fecha = \"2025-09-20\"\n",
    "year, month, day = map(int, fecha.split(\"-\"))\n",
    "\n",
    "# Generar features para esa fecha\n",
    "# En tu caso, solo agregados históricos por mes y año\n",
    "# Aquí podemos usar medias históricas del mes de septiembre\n",
    "df_month_avg = df_hotel_features.filter(col(\"mes\") == month).agg(\n",
    "    avg(\"hotel_viajeros\").alias(\"hotel_viajeros\"),\n",
    "    avg(\"hotel_pernoctaciones\").alias(\"hotel_pernoctaciones\"),\n",
    "    avg(\"hotel_estancia_media\").alias(\"hotel_estancia_media\"),\n",
    "    avg(\"avg_ocupacion\").alias(\"avg_ocupacion\"),\n",
    "    avg(\"hotel_availability_score\").alias(\"hotel_availability_score\")\n",
    ").collect()[0]\n",
    "\n",
    "# Crear fila para predicción\n",
    "future_row = Row(\n",
    "    año=year,\n",
    "    mes=month,\n",
    "    hotel_viajeros=df_month_avg[\"hotel_viajeros\"],\n",
    "    hotel_pernoctaciones=df_month_avg[\"hotel_pernoctaciones\"],\n",
    "    hotel_estancia_media=df_month_avg[\"hotel_estancia_media\"],\n",
    "    avg_ocupacion=df_month_avg[\"avg_ocupacion\"],\n",
    "    hotel_availability_score=df_month_avg[\"hotel_availability_score\"]\n",
    ")\n",
    "\n",
    "# Convertir a DataFrame Spark\n",
    "future_df = spark.createDataFrame([future_row])\n",
    "\n",
    "model_uri = f\"file:///D:/Quim/Documents/quim documents/Master/TFM/TravelMind/ml_models/mlruns/279040930689046531/467a75a48f5a4cf48f7b4627a9daf6df/artifacts/spark_xgb_model\"\n",
    "loaded_model = mlflow.spark.load_model(model_uri)\n",
    "# Aplicar pipeline entrenado\n",
    "prediction = model.transform(future_df)\n",
    "\n",
    "# Mostrar resultado\n",
    "prediction.select(\n",
    "    \"año\", \"mes\", \"prediction\", \"probability\"\n",
    ").show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TravelMind-69GU69Sq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
