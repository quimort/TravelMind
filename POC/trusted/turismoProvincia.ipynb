{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd12423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import upper, col\n",
    "import os\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4b13b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Reading landing Parquet from c:\\Users\\joaqu\\OneDrive\\Documents\\AAmaster_UPC\\TFM\\TravelMind\\POC\\landing\\data\\landing\\turismo_Provincia\n",
      "→ Writing trusted CSV to c:\\Users\\joaqu\\OneDrive\\Documents\\AAmaster_UPC\\TFM\\TravelMind\\POC\\trusted\\data\\trusted\\turismo_Provincia_clean_csv\n",
      "✅ Trusted zone load complete (CSV).\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import upper, col\n",
    "\n",
    "# Pin Python for Spark workers & driver\n",
    "os.environ[\"PYSPARK_PYTHON\"]        = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "def main():\n",
    "    # 1) Determine base POC folder (one level up from CWD)\n",
    "    cwd = os.getcwd()  \n",
    "    poc_root = os.path.abspath(os.path.join(cwd, \"..\"))\n",
    "\n",
    "    # 2) Landing parquet actually lives under POC/landing/data/landing/turismo_Provincia\n",
    "    landing_dir = os.path.join(poc_root, \"landing\", \"data\", \"landing\", \"turismo_Provincia\")\n",
    "    print(\"→ Reading landing Parquet from\", landing_dir)\n",
    "\n",
    "    # 3) Start Spark session\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "          .appName(\"TrustedCSVLoad\")\n",
    "          .config(\"spark.master\", \"local[*]\")\n",
    "          .config(\"spark.driver.memory\", \"2g\")\n",
    "          .getOrCreate()\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # 4) Read landing Parquet\n",
    "        if not os.path.isdir(landing_dir):\n",
    "            raise FileNotFoundError(f\"Landing dir not found: {landing_dir}\")\n",
    "        df = spark.read.parquet(landing_dir)\n",
    "\n",
    "        # 5) Clean & normalize\n",
    "        df_clean = df.dropna()\n",
    "        for c in [\"CCAA_ORIGEN\",\"PROVINCIA_ORIGEN\",\"CCAA_DESTINO\",\"PROVINCIA_DESTINO\"]:\n",
    "            df_clean = df_clean.withColumn(c, upper(col(c)))\n",
    "\n",
    "        # 6) Write trusted CSV under POC/trusted/data/trusted/…\n",
    "        tgt_dir = os.path.join(poc_root, \"trusted\", \"data\", \"trusted\", \"turismo_Provincia_clean_csv\")\n",
    "        os.makedirs(tgt_dir, exist_ok=True)\n",
    "        print(\"→ Writing trusted CSV to\", tgt_dir)\n",
    "\n",
    "        (\n",
    "            df_clean\n",
    "              .repartition(1)\n",
    "              .write\n",
    "              .mode(\"overwrite\")\n",
    "              .option(\"header\", True)\n",
    "              .csv(tgt_dir)\n",
    "        )\n",
    "\n",
    "        print(\"✅ Trusted zone load complete (CSV).\")\n",
    "\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ffa9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Reading from Iceberg landing…\n"
     ]
    },
    {
     "ename": "ParseException",
     "evalue": "\n[INVALID_IDENTIFIER] The identifier spark-warehouse is invalid. Please, consider quoting it with back-quotes as `spark-warehouse`.(line 1, pos 13)\n\n== SQL ==\nlanding.spark-warehouse.turismo_Provincia\n-------------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParseException\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m→ Reading from Iceberg landing…\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     df = \u001b[43miceberg_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlanding.spark-warehouse.turismo_Provincia\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m      7\u001b[39m     iceberg_spark.stop()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joaqu\\.virtualenvs\\TravelMind--WcSvJCM\\Lib\\site-packages\\pyspark\\sql\\session.py:1667\u001b[39m, in \u001b[36mSparkSession.table\u001b[39m\u001b[34m(self, tableName)\u001b[39m\n\u001b[32m   1636\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtable\u001b[39m(\u001b[38;5;28mself\u001b[39m, tableName: \u001b[38;5;28mstr\u001b[39m) -> DataFrame:\n\u001b[32m   1637\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns the specified table as a :class:`DataFrame`.\u001b[39;00m\n\u001b[32m   1638\u001b[39m \n\u001b[32m   1639\u001b[39m \u001b[33;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1665\u001b[39m \u001b[33;03m    +---+\u001b[39;00m\n\u001b[32m   1666\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1667\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtableName\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joaqu\\.virtualenvs\\TravelMind--WcSvJCM\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joaqu\\.virtualenvs\\TravelMind--WcSvJCM\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mParseException\u001b[39m: \n[INVALID_IDENTIFIER] The identifier spark-warehouse is invalid. Please, consider quoting it with back-quotes as `spark-warehouse`.(line 1, pos 13)\n\n== SQL ==\nlanding.spark-warehouse.turismo_Provincia\n-------------^^^\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Step 1: Read with Iceberg-aware session ---\n",
    "iceberg_spark = utils.create_context()\n",
    "try:\n",
    "    print(\"→ Reading from Iceberg landing…\")\n",
    "    df = iceberg_spark.table(\"landing.spark-warehouse.turismo_Provincia\")\n",
    "finally:\n",
    "    iceberg_spark.stop()\n",
    "\n",
    "# --- Step 2: Transform & write CSV with a pure-Java session ---\n",
    "pure_spark = (\n",
    "    SparkSession.builder\n",
    "      .appName(\"TrustedCSVLoad\")\n",
    "      .config(\"spark.master\", \"local[*]\")\n",
    "      .config(\"spark.driver.memory\", \"2g\")\n",
    "      .getOrCreate()\n",
    ")\n",
    "\n",
    "tgt_dir = \"./data/trusted/turismo_Provincia_clean_csv\"\n",
    "os.makedirs(tgt_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    print(\"→ Dropping nulls & upper-case…\")\n",
    "    df_clean = df.dropna()\n",
    "    for c in [\"CCAA_ORIGEN\",\"PROVINCIA_ORIGEN\",\"CCAA_DESTINO\",\"PROVINCIA_DESTINO\"]:\n",
    "        df_clean = df_clean.withColumn(c, upper(col(c)))\n",
    "\n",
    "    print(\"→ Writing CSV to\", tgt_dir)\n",
    "    (\n",
    "      df_clean\n",
    "        .repartition(1)\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"header\", True)\n",
    "        .csv(tgt_dir)\n",
    "    )\n",
    "    print(\"✅ CSV write complete.\")\n",
    "finally:\n",
    "    pure_spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TravelMind--WcSvJCM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
